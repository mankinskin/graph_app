@article{parsing2009speech,
  title={Speech and language processing},
  author={Parsing, Constituency},
  journal={Power Point Slides},
  year={2009}
}
@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{camburu2019make,
  title={Make up your mind! adversarial generation of inconsistent natural language explanations},
  author={Camburu, Oana-Maria and Shillingford, Brendan and Minervini, Pasquale and Lukasiewicz, Thomas and Blunsom, Phil},
  journal={arXiv preprint arXiv:1910.03065},
  year={2019}
}
@article{filippova2020controlled,
  title={Controlled hallucinations: Learning to generate faithfully from noisy data},
  author={Filippova, Katja},
  journal={arXiv preprint arXiv:2010.05873},
  year={2020}
}
@article{zouhar2023formal,
  title={A formal perspective on byte-pair encoding},
  author={Zouhar, Vil{\'e}m and Meister, Clara and Gastaldi, Juan Luis and Du, Li and Vieira, Tim and Sachan, Mrinmaya and Cotterell, Ryan},
  journal={arXiv preprint arXiv:2306.16837},
  year={2023}
}
@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}
@article{south2023transparency,
  title={Transparency by Design for Large Language Models},
  author={South, Tobin and Mahari, Robert and Pentland, Alex},
  journal={Computational Legal Futures, Network Law Review.(2023)},
  year={2023}
}
@article{ji2023survey,
  title={Survey of hallucination in natural language generation},
  author={Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
  journal={ACM Computing Surveys},
  volume={55},
  number={12},
  pages={1--38},
  year={2023},
  publisher={ACM New York, NY}
}
@article{orosi2018simple,
  title={A simple derivation of Faulhaber’s formula},
  author={Orosi, Greg},
  journal={Appl. Math. E-Notes},
  volume={18},
  pages={124--126},
  year={2018}
}
@book{tomita1991generalized,
  title={Generalized LR parsing},
  author={Tomita, Masaru},
  year={1991},
  publisher={Springer Science \& Business Media}
}
@inproceedings{lang1974deterministic,
  title={Deterministic techniques for efficient non-deterministic parsers},
  author={Lang, Bernard},
  booktitle={International Colloquium on Automata, Languages, and Programming},
  pages={255--269},
  year={1974},
  organization={Springer}
}
@inproceedings{chefer2021transformer,
  title={Transformer interpretability beyond attention visualization},
  author={Chefer, Hila and Gur, Shir and Wolf, Lior},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={782--791},
  year={2021}
}
@ARTICLE{6796337,
  author={Schmidhuber, Jürgen},
  journal={Neural Computation}, 
  title={Learning to Control Fast-Weight Memories: An Alternative to Dynamic Recurrent Networks}, 
  year={1992},
  volume={4},
  number={1},
  pages={131-139},
  doi={10.1162/neco.1992.4.1.131}
}
@Article{math11102320,
AUTHOR = {Lee, Minhyeok},
TITLE = {A Mathematical Investigation of Hallucination and Creativity in GPT Models},
JOURNAL = {Mathematics},
VOLUME = {11},
YEAR = {2023},
NUMBER = {10},
ARTICLE-NUMBER = {2320},
URL = {https://www.mdpi.com/2227-7390/11/10/2320},
ISSN = {2227-7390},
ABSTRACT = {In this paper, we present a comprehensive mathematical analysis of the hallucination phenomenon in generative pretrained transformer (GPT) models. We rigorously define and measure hallucination and creativity using concepts from probability theory and information theory. By introducing a parametric family of GPT models, we characterize the trade-off between hallucination and creativity and identify an optimal balance that maximizes model performance across various tasks. Our work offers a novel mathematical framework for understanding the origins and implications of hallucination in GPT models and paves the way for future research and development in the field of large language models (LLMs).},
DOI = {10.3390/math11102320}
}
@article{nevill1997identifying,
  title={Identifying hierarchical structure in sequences: A linear-time algorithm},
  author={Nevill-Manning, Craig G and Witten, Ian H},
  journal={Journal of Artificial Intelligence Research},
  volume={7},
  pages={67--82},
  year={1997}
}
@article{larsson2000off,
  title={Off-line dictionary-based compression},
  author={Larsson, N Jesper and Moffat, Alistair},
  journal={Proceedings of the IEEE},
  volume={88},
  number={11},
  pages={1722--1732},
  year={2000},
  publisher={IEEE}
}
@article{feng1987progressive,
  title={Progressive sequence alignment as a prerequisitetto correct phylogenetic trees},
  author={Feng, Da-Fei and Doolittle, Russell F},
  journal={Journal of molecular evolution},
  volume={25},
  pages={351--360},
  year={1987},
  publisher={Springer}
}
@article{katz1987estimation,
  title={Estimation of probabilities from sparse data for the language model component of a speech recognizer},
  author={Katz, Slava},
  journal={IEEE transactions on acoustics, speech, and signal processing},
  volume={35},
  number={3},
  pages={400--401},
  year={1987},
  publisher={IEEE}
}
@article{li2022hierarchical,
  title={A hierarchical n-gram framework for zero-shot link prediction},
  author={Li, Mingchen and Chen, Junfan and Mensah, Samuel and Aletras, Nikolaos and Yang, Xiulong and Ye, Yang},
  journal={arXiv preprint arXiv:2204.10293},
  year={2022}
}
@inproceedings{koutra2011algorithms,
  title={Algorithms for graph similarity and subgraph matching},
  author={Koutra, Danai and Parikh, Ankur and Ramdas, Aaditya and Xiang, Jing},
  booktitle={Proc. Ecol. inference conf},
  volume={17},
  year={2011},
  organization={Citeseer}
}
@article{VaswaniSPUJGKP17,
  author       = {Ashish Vaswani and
                  Noam Shazeer and
                  Niki Parmar and
                  Jakob Uszkoreit and
                  Llion Jones and
                  Aidan N. Gomez and
                  Lukasz Kaiser and
                  Illia Polosukhin},
  title        = {Attention Is All You Need},
  journal      = {CoRR},
  volume       = {abs/1706.03762},
  year         = {2017},
  url          = {http://arxiv.org/abs/1706.03762},
  eprinttype    = {arXiv},
  eprint       = {1706.03762},
  timestamp    = {Sat, 23 Jan 2021 01:20:40 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{maynez-etal-2020-faithfulness,
    title = "On Faithfulness and Factuality in Abstractive Summarization",
    author = "Maynez, Joshua  and
      Narayan, Shashi  and
      Bohnet, Bernd  and
      McDonald, Ryan",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.173",
    doi = "10.18653/v1/2020.acl-main.173",
    pages = "1906--1919",
    abstract = "It is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling and story generation. In this paper we have analyzed limitations of these models for abstractive document summarization and found that these models are highly prone to hallucinate content that is unfaithful to the input document. We conducted a large scale human evaluation of several neural abstractive summarization systems to better understand the types of hallucinations they produce. Our human annotators found substantial amounts of hallucinated content in all model generated summaries. However, our analysis does show that pretrained models are better summarizers not only in terms of raw metrics, i.e., ROUGE, but also in generating faithful and factual summaries as evaluated by humans. Furthermore, we show that textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria.",
}
@inproceedings{
dinan2018wizard,
title={Wizard of Wikipedia: Knowledge-Powered Conversational Agents},
author={Emily Dinan and Stephen Roller and Kurt Shuster and Angela Fan and Michael Auli and Jason Weston},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=r1l73iRqKm},
}
@article{shuster2021retrieval,
  title={Retrieval augmentation reduces hallucination in conversation},
  author={Shuster, Kurt and Poff, Spencer and Chen, Moya and Kiela, Douwe and Weston, Jason},
  journal={arXiv preprint arXiv:2104.07567},
  year={2021}
}
@article{weizenbaum1966eliza,
  title={ELIZA—a computer program for the study of natural language communication between man and machine},
  author={Weizenbaum, Joseph},
  journal={Communications of the ACM},
  volume={9},
  number={1},
  pages={36--45},
  year={1966},
  publisher={ACM New York, NY, USA}
}
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}
@misc{haim2022reconstructing,
      title={Reconstructing Training Data from Trained Neural Networks}, 
      author={Niv Haim and Gal Vardi and Gilad Yehudai and Ohad Shamir and Michal Irani},
      year={2022},
      eprint={2206.07758},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{dietterichs,
  title={What’s Wrong with Large Language Models and What We Should be Building Instead},
  author={Dietterich, Thomas G}
}
@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}
@article{petroni2019language,
  title={Language models as knowledge bases?},
  author={Petroni, Fabio and Rockt{\"a}schel, Tim and Lewis, Patrick and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander H and Riedel, Sebastian},
  journal={arXiv preprint arXiv:1909.01066},
  year={2019}
}