

%using a complete substring relation and frequency counts of repeating substrings, derived from an input corpus by a grammar induction procedure.

%Neural models of the latest generation have proved their ability to learn complex conceptual relationships in extremely large corpora of natural language using a mechanism called self-attention. Given the convincing generative results of these models, new interest in applying these models at a larger scale has sparked. Natural language model now make considerable contributions to creative writing, knowledge representation and extraction, language translation and programming. Despite the impressive advancement of the field, neural language models remain fundamentally opaque, even to knowledgable users, and give no guarantees regarding the truthfulness of their responses and often produce inconsistent outputs. Additionally, neural models are inherently difficult to modify in a targeted manner after they have been trained, further increasing the cost of building reliable language models using neural methods.
%
%In this paper we investigate the specific strengths and weaknesses of attention-based neural language models in comparison to classic n-gram based modelling approaches. We propose the use of explicit relationships between lexical elements to address the problems of neural models and augment a classical n-gram model with a contextual relationship structure to capture information similar to the attention weights used in neural models. Further we devise a compression scheme on the resulting hierarchical n-gram graph to increase the capacity to store long-range dependencies.


%There are however fundamental disadvantages of neural models over count based models.

Plan:
\begin{itemize}
\item x define target operations on structure as solution to problems
\item x define string containment index with these operations
\item x define compressed index for efficient structure
\item x define how to predict context given tokens
\end{itemize}


Target operations:
\begin{itemize}
\item get all contexts for any substring of corpus
\item reconstruct corpus
\item change corpus after training
\end{itemize}

\section{Problem}
\begin{itemize}
    \item NLMs suffer from hallucinations
    \item NLMs are hard to explain
    \item NLMs are hard to update
    \item NLMs are hard to predict
    \item The need for controllable language models
\end{itemize}

\section{Solution}
\begin{itemize}
    \item use a model without hallucinations for reconstrucing a training corpus
    \item use an explicit symbolic model to help with explainability and updates
    \item allow for creative outputs under controlled conditions
\end{itemize}

\section{Graphics}
\begin{itemize}
    \item visualize attention matrix of transformers
    \item ambiguous grammar to shared packet parse forest
\end{itemize}

\section{Background}

\subsection{Information Retrieval}

\subsection{Hallucinations and Creativity}
\begin{itemize}
    \item models can generate inconsistent outputs
    \item required tradeoff to enable novel creative outputs
\end{itemize}

\subsection{n-gram Language models}
Early research of language models, 
\begin{itemize}
    \item no hallucinations or creativity, instead issues with sparsity
    \item long range relationships require large n-grams with high cost
    \item hierarchical n-grams can use multiple context windows widths
    \item use markov processes to predict new tokens
    \item can be smoothed to estimate unknown sequences for creativity, at cost of hallucinations
\end{itemize}
\paragraph{Katz Back-Off Model}
\begin{itemize}
    \item able to find largest n-gram to predict next token
\end{itemize}
\paragraph{Markov-Processes}
\begin{itemize}
    \item only require looking at one node at a time to make prediction
\end{itemize}
\paragraph{Probabilistic Context-Free Grammars}
\begin{itemize}
    \item apply stochastic weighing to grammar parsing and generation
\end{itemize}
\paragraph{Counting}
\begin{itemize}
    \item 
\end{itemize}
\subsection{Compression}
\begin{itemize}
    \item relies on replacing repetitions with special symbols to avoid redundancy
    \item useful to decrease hierarchical n-gram size
    \item compressed representation is a grammar generating the uncompressed input
    \item form of grammar induction
\end{itemize}
\subsection{Grammar Induction}
\begin{itemize}
    \item
\end{itemize}
\subsection{Explainability of Language Models}
\begin{itemize}
    \item provides explanations for predictions
    \item makes it easy to find cause for behavior
\end{itemize}

\begin{itemize}
    \item \textbf{Induction:} Construct new rules to represent a given string of symbols
    \item \textbf{Search:} Find grammar rules related to a given string of symbols
\end{itemize}


\paragraph{GLR-Parser}
The Generalized Left Right (GLR) Parser is a parsing algorithm that is able to parse 
ambiguous
and non-deterministic grammars \footcite{lang1974deterministic}. It is able to do so by tracking multiple parse trees in a graph-structured stack.

\paragraph{Shared-Packet Parse Forests}

\paragraph{Grammar Induction}

\section{Related Methods}

\paragraph{Sequitur}
The Sequitur algorithm \footcite{nevill1997identifying} is a method of grammar induction 
usable for data compression and discovery of lexical structure. The algorithm 
recursively replaces repeated sections in a given input string with new grammar rules, iteratively extending a context-free grammar yielding the original string. The Sequitur algorithm enforces two constraints during its operation:
\begin{itemize}
    \item \textit{Digram Uniqueness}: each subsequent pair of symbols occurs at most once in all rules of the grammar. This constraint prevents redundant digrams in the grammar's rules.
    \item \textit{Rule Utility}: Every rule is used at least twice. This constraint prevents redundant rules.
\end{itemize}
These constraints not only reduce the size of the grammar, allowing for compression, but also capture some of the lexical structure in the original string. Although the algorithm operates in linear space and time, it does not capture all lexical relationships of repeated substrings if there exist overlapping repetitions in the original string. The Sequitur algorithm always replaces the left-most repetition with a non-terminal symbol and does not consider any repetitions starting from within this replaced substring. This causes relationships of some repeated phrases to not be captured, which poses a problem when looking for the complete distribution of contexts of arbitrary sequences, as not all sequences are fully linked with their contexts.
To relieve this issue we introduce a new constraint on the grammar induction, which enforces that for each 
\paragraph{Re-Pair}
\paragraph{R-Trees}
\paragraph{Top-Trees}
\paragraph{Byte-Pair Encoding}
\paragraph{Sequence Alignment}
\paragraph{Hierarchical n-grams}
\paragraph{Recursive Hypergraph}

\section{Analysis}
\subsection{Search}
\subsection{Insert}

\section{Application}
\subsection{Reconstruction}
\subsection{Update Training Data}
\subsection{Reconstruct Context}
\subsection{Similarity}
\subsection{Predict Context}

\newcommand{\convexpath}[2]{
[   
    create hullnodes/.code={
        \global\edef\namelist{#1}
        \foreach [count=\counter] \nodename in \namelist {
            \global\edef\numberofnodes{\counter}
            \node at (\nodename) [name=hullnode\counter,draw=none] {};
        }
        \node at (hullnode\numberofnodes) [name=hullnode0,draw=none] {};
        \pgfmathtruncatemacro\lastnumber{\numberofnodes+1}
        \node at (hullnode1) [name=hullnode\lastnumber,draw=none] {};
    },
    create hullnodes
]
($(hullnode1)!#2!-90:(hullnode0)$)
\foreach [
    evaluate=\currentnode as \previousnode using \currentnode-1,
    evaluate=\currentnode as \nextnode using \currentnode+1
    ] \currentnode in {1,...,\numberofnodes} {
  let
    \p1 = ($(hullnode\currentnode)!#2!-90:(hullnode\previousnode)$),
    \p2 = ($(hullnode\currentnode)!#2!90:(hullnode\nextnode)$),
    \p3 = ($(\p1) - (hullnode\currentnode)$),
    \n1 = {atan2(\y3,\x3)},
    \p4 = ($(\p2) - (hullnode\currentnode)$),
    \n2 = {atan2(\y4,\x4)},
    \n{delta} = {-Mod(\n1-\n2,360)}
  in 
    {-- (\p1) arc[start angle=\n1, delta angle=\n{delta}, radius=#2] -- (\p2)}
}
-- cycle
}
\newcommand{\nodes}[2][0]{
    %\StrCount{#1,}{,}[\len]
    \global\edef\namelist{#2}
    \foreach[count=\ci] \chr in \namelist {
        %\StrChar{#1}{\ci}[\chr],
        \node(\chr) at (\ci, #1) {\chr};
    }
}
\def\splicelist#1{
\StrCount{#1}{,}[\numofelem]
\ifnum\numofelem>0\relax
    \StrBefore[1]{#1}{,}[\listhead]%
    \StrBehind[1]{#1}{,}[\listtail]%
    \StrBehind[\numofelem]{#1}{,}[\listlast]%
\else
    \let\listhead#1%
    \let\listlast#1%
    \def\listtail{N/A}
\fi
}
\newcommand{\hyperedge}[1]{
    \def\namelist{#1},
    \splicelist{\namelist}
    \draw \foreach \n [remember=\n as \lastn(initially \listhead)] in \listtail {
        (\lastn) edge[->] (\n)
    };
}

\begin{figure}
\centering
\begin{tikzpicture}[
    y=.7cm, x=1cm,
    every edge/.append style={thick}
]

%\node (b) at (1,0) {b};
%\node (c) at (2,0) {c};
%\node (d) at (3,0) {d};
%\node (e) at (4,0) {e};
%\node (f) at (5,0) {f};
\nodes{a, b, c, d, e, f}
\hyperedge{a, b, c, d, e, f}

\filldraw[line width=.5mm, blue,opacity=0.3] \convexpath{a, b, c, d}{8pt};
\filldraw[line width=.5mm, red,opacity=0.3] \convexpath{c, d, e, f}{8pt};

\end{tikzpicture}
\caption{A hypergraph with two hyperedges}
\end{figure}

\subsection{Tracing Overlaps}

To find these overlaps as efficiently as possible, we perform a search locally around symbols we have
previously found. This reduces the search space and makes use of previously computed results.

After we have parsed the first largest symbol, we want to find the next largest overlap with this symbol.
So we search top-down through all of the postfixes of the symbol from largest to smallest, and for each
we try to parse the word resulting from appending the postfix with the remaining tokens from the input word.
We either never find a symbol representing any overlap, in which case we simply continue parsing the remaining input,
or we find an overlap. An overlap starts with some postfix from the former symbol, i.e. it
uses it as a prefix in one of its rules. To build the resulting set of production rules, we need to complete the
context in backward direction, to represent the sequence before the overlap. For this we can make use of the parse
tree of the former symbol of which we searched a postfix. By accumulating the backward context along all of the
levels of its parse tree, we can create a new production rule representing the backward context of the overlap. (PROOF)

In the resulting state we have two bands or two production rules, each representing a prefix of the input word of
different lengths. We now continue to search for overlaps of the now latest symbol, i.e. the former overlap we have
just found. Overlaps for this symbol could also overlap with the symbol we were previously finding overlaps for.
However we can disprove that this can be the case if the production rule of the overlapping entry, i.e. the one
starting with the postfix of the previous symbol, contains more than two symbols. This follows from the condition 
that no sequence of symbols may appear twice anywhere in the entire grammar. Only if there is exactly one symbol 
following the former postfix, may that same sequence appear in another larger symbol. We can use this knowledge to 
decide our next steps.

In the case where there are at least two symbols following the first in the rule, we know that any overlaps must begin 
after the end of the last symbol we overlapped. If there is only one following symbol, however, there may be an overlap 
with the current symbol and also the previous symbol. If such a symbol exists, there must be a production rule in the 
current symbol with a postfix larger than that of the rule we entered the current symbol through, because of the 
maximality constraint. We can use this postfix to find the next overlap, however the postfix may not be expandable into 
the following tokens. We search throught the postfixes in descending order. We will eventually encounter the postfix of 
the rule we entered through. We use it to complete the rule with the first symbol we parsed by appending it. If the 
postfix is expandable, we append the extension. This extension is then overlapping with the current symbol and we can 
move on to repeat the algorithm inside the extension.

Earlier, we mentioned the case where the entry rule of a symbol contained more than two symbols, and that in this case 
we know that there can be no overlap with the current symbol starting at or before the end of the last symbol. But there 
can still be an overlap with the current symbol which starts after the end of the last symbol. In case we don't find any 
overlaps, we will simply create a new symbol for the postfix of the current symbol after the entry from the last symbol 
and append it to the band with the last symbol. We get two bands with different rules but generating the same strings 
(which are of equal length). We create a new symbol for these two bands and replace them with it, because we can not have 
two boundaries at the same string position in any rules of one symbol. 

\paragraph{Ambiguous Grammars}

\paragraph{Grammar Traversal}
%Essential to using the model is the search of known sequences.
%In this section we will describe how the model can be used to parse an input string,
%effectively searching the model for known information about the string.
This provides us with one example of how the nodes and edges of the graph can be
iteratively visited and worked on.
In principle the parsing stage employs a bottom up, Generalized-Left-Right(GLR)-Parsing 
process, which uses a graph-structured stack (GSS) to traverse all possible parse trees 
of the input.
%\begin{align}
%
%\end{align}

\paragraph{GLR-Parsing}
The algorithm works by walking upwards over all parent nodes of the starting symbol of 
the input.
For each parent, it attempts to compare the remaining input with the remaining input of 
the parent node.
Abstractly, the algorithm will continue to expand nodes upwards until it finds a parent 
with a matching
continuation (or exhausted all known contexts and reports a mismatch).
When a matching continuation is found, all alternative parse trees can be abandoned as 
they all lead either
to different contexts than the smallest unique one, or must lead to the same parent.
(convincing formal proof)
This can be derived from the equal children axiom and the fact that breadth first search 
will search parents
in a width-ascending order.

All sequences known to the model can be found by starting at any symbol and traversing 
its parent contexts in a bottom up iterative process. When given a search query in the 
form of a sequence, we can start with any of the symbols and match the surrounding 
symbols in the query with the contexts stored in the model grammar. This is basically a 
parsing problem where we try to parse a given word of a grammar.

Not all of the symbols in the query need to be atomic symbols. We can match non-atomic symbols the same way we match symbols in model grammar rules. We provide an overview of the algorithm and a more detailed pseudo-code representation below. The result of the parsing process is a rooted sub-graph structure containing the paths traversed during the parsing process, with the smallest symbol containing the entire known sequence as a root. 

\subsection{Construction}

\paragraph{Reusing partitions}
With navigational information from search, an infrequent sequence can be compressed into an identifying symbol. This is needed to compress larger sequences of smaller known sequences.

During search, paths over the grammar are traced and combined to a sub-graph visiting all leaf nodes of a subsequence in a root symbol.

\paragraph{Partitioning Rules}
The sub-graph structure resulting from a search contains all of the information needed to create a new symbol representing the identified sequence. The paths contained in the sub-graph point to the exact locations where the partition intersects rules in the grammar, which we need to modify to support the new symbol about to be added to the index.

The basic idea is to find all of the locations where intersected symbols need to be split into partitions which can then be used to build larger symbols without repeating sequences in any rules (which would violate invariant 3).

\subsection{Counting}
Upper bound for repeated substrings in a string of length $N$ over an alphabet of $k$ symbols:


\subsection{Consuming Sequences}
In order to consume new sequences and add them to the model, we require a set of algorithms on the grammar structure described in the remainder of this section. The process can be divided in three major stages, each working on the results of the previous stage:
\begin{enumerate}
\item Search largest known partition at position
\item Join partition sub-graph into new symbol
\item Join found partitions into new symbol for the entire input sequence
\end{enumerate}

When reading new symbols, we want to construct them in a way that upholds certain properties to make the resulting graph structure space efficient and useful for traversal:
\begin{itemize}
    \item \textbf{Digram Uniqueness} Every string of symbols in all rules in the graph occurs at most once.
    \begin{gather*}
        \forall r_a, r_b \in R, i \in \{ 0, ..., |r_a| - 2 \}, j \in \{ 0, ..., |r_b| - 2 \}:\\
        a \neq b \lor i \neq j \Longrightarrow (r_a[i], r_a[i + 1]) \neq (r_b[j], r_b[j + 1])
    \end{gather*}

    \item \textbf{Deterministic Expansions} Each symbol represents a single string. Every rule of the symbol produces the same string.
    \begin{align*}
        \forall s \in V, r_i, r_j \in R_s: \texttt{expand}_G(r_i) = \texttt{expand}_G(r_j)
    \end{align*}

    \item \textbf{Edge Completeness} Every symbol must contain all largest symbols representing any of its sub-strings in its rules.
    \begin{align*}
        \forall a, s \in V&: \texttt{expand}_G(a) \text{ substring of } \texttt{expand}_G(s) \\
        &\nexists b \in V: \texttt{expand}_G(a) \text{ substring of } \texttt{expand}_G(b) \\
        %%&\land  \texttt{expand}_G(b) \text{ substring of } \texttt{expand}_G(s) \RightArrow \exists r \in s: a \in r
    \end{align*}

    \item \textbf{No Shared Boundaries} All boundaries between two rule symbols in a symbol represent unique positions in the expanded string.
    \begin{gather*}
    \end{gather*}
\end{itemize}
\paragraph*{Language Modelling}
Let $\Sigma$ be the token alphabet of a language $L \subseteq \Sigma^*$ and let $O \subseteq L$ be a given training corpus of positive examples from $L$.
A language model $M$ estimates the unknown language $L$.
A conditional probability distribution for the next token $t_{n+1} \in \Sigma$ given a surrounding context of $n$ fixed tokens, $(t_i)_{i = 1}^n \in \Sigma^n$:
\begin{align*}
    P_M(t_{n+1} \mid t_1, \ldots, t_n)
\end{align*}

When this conditional distribution is known, it is possible to estimate probabilities for given sequences of arbitrary length by conditioning each token on the sequence of previous tokens. The following equality holds:
\begin{align*}
    P(t_1, t_2, \ldots, t_N) =\  &P(t_1) * P(t_2  \mid t_1) * P(t_3 \mid t1, t2)\\
                          *\  &\ldots\\
                          *\  &P(t_N \mid t_1, \ldots, t_{N-1})
\end{align*}
In n-Gram models, these individual probabilities are estimated by counting the frequency of each configuration of $n$ tokens. Let $C(t)$ be the absolute frequency or count of the given n-gram $t$. Then the conditional probability is estimated with the relative frequency of the given $n$-gram and its $n-1$ prefix:
\begin{align*}
    P(t_i \mid t_{i-n+1}, \ldots, t_{i-1}) = \frac{C(t_{i-n+1}, \ldots, t_i)}{C(t_{i-n+1}, \ldots, t_{i-1})}
\end{align*}

Because larger $n$-grams contain smaller $n$-grams aswell

A grammar is a formalism of rules for constructing or recognizing elements of a set. Usually these elements are continuous sequences or strings. The set of strings accepted by a grammar is called the language of the grammar. We will use the hierarchical structure of a recursive hypergraph as a grammar to model the target language. The target language will be given by a corpus of examples. In this paper, we will focus on languages of strings of characters, however the mechanisms in this paper can be applied to arbitrary tokens. We also believe that the framework can be expanded to more complex graph structures than linear lists, however this remains to be investigated by future research.
The structure is induced from positive examples of the target language and essentially compresses them into a single grammar. The process is therefore unsupervised and can be applied directly on a given set of observations to be modelled.
In abstract terms the induction procedure populates a given grammar $G$ with new rules for a given example $x$ by first compressing $x$ using the grammar and then inserting the compressed representation into the grammar as a new rule:
\begin{align*}
    \texttt{INSERT}(\texttt{COMPRESS}(x, G), G)
\end{align*}
It is important to note that the compressed structure is not primarily designed to reduce the size of the input, but to improve the efficiency of specific queries that are common in stochastic language modelling and semantic representation while retaining the exact lexical structure of the input.

There is a specific set of rules that is placed on the structure of the grammar, which ensure that the structure is both efficient to store and navigate.

In the hyper

%% number of tokens to store
\noindent With a string containment graph, we can easily find all of the strings which appear within other strings and thus we can also easily find which strings appear close to other strings. It is also possible to count how frequently individual strings appear by counting the number of different offsets each substring appears at in any superstring.

For a corpus of length $N$ the number of all possible substrings of length $n$ is $N - n + 1$. If we store every string of length n directly, the total number of tokens we have to store is:
\begin{align*}
    \sum_{n=1}^{N}{n * (N - n + 1)}
    &= (N + 1)\sum_{n=1}^{N}{n} -
    \sum_{n=1}^{N}{n^2}\\
    &= (N + 1)\frac{N^2 + N}{2} - \frac{(N^2 + N)(2N + 1)}{6}\\
    &= (N + 1 - \frac{2N + 1}{18})\frac{N^2 + N}{2}
\end{align*}
which is of the order $O(N^3)$. In addition to the strings we need to store all the substring relations which amount to exactly the number of substrings for each string of length $n$, which is of the order $O(n^2)$. If we need to store this for every substring of the corpus with length $N$, the number of edges we need to store is of the order $O(N^4)$.

\noindent
This leaves us with a space complexity of $O(N^2 + k)$ to store the nodes of the graph, where $k$ is the number of tokens in the alphabet $\Sigma$. However, as $k$ can be bounded by $N$ (there can not be more tokens than the length of the corpus), this is simply $O(N^2)$.