
\chapter{Introduction}

Neural models of the latest generation have proved their ability to learn complex conceptual relationships in extremely large corpora of natural language using a mechanism called self-attention. Given the convincing generative results of these models, new interest in applying these models at a larger scale has sparked. Natural language model now make considerable contributions to creative writing, knowledge representation and extraction, language translation and programming. Despite the impressive advancement of the field, neural language models remain fundamentally opaque, even to knowledgable users, and give no guarantees regarding the truthfulness of their responses and often produce inconsistent outputs. Additionally, neural models are inherently difficult to modify in a targeted manner after they have been trained, further increasing the cost of building reliable language models using neural methods.

In this paper we investigate the specific strengths and weaknesses of attention-based neural language models in comparison to classic N-Gram based modelling approaches. We propose the use of explicit relationships between lexical elements to address the problems of neural models and augment a classical N-Gram model with a contextual relationship structure to capture information similar to the attention weights used in neural models. Further we devise a compression scheme on the resulting hierarchical N-Gram graph to increase the capacity to store long-range dependencies.


There are however fundamental disadvantages of neural models over count based models.

\dfn{Interpretability}{
    The interpretability of a language model quantifies the degree a human can understand the model's decisions and parameters.
}
\dfn{Updatability}{
    The updatability of a language model refers to the ability to update a trained language model with new knowledge or modify its behavior.
}
\dfn{Controllability}{

}
\section{Hallucinating Language Models}

\section{Opaque Language Models}

\section{Unpredictable Language Models}

\section{Distributional Semantics}

In the research field of distributional semantics, the similarity between linguistic 
items is defined as the the similarity of their respective context distributions in a 
given text corpus. The so-called distributional hypothesis postulates that "items which 
appear in similar contexts are similar in their semantic meaning" or "words are 
characterized by the company they keep".
Previous methods have mainly focussed on embedding tokens from sequences in a 
high-dimensional vector space in a way that captures the distributional features of each 
token. These token embeddings can then be used to estimate the most likely context tokens 
and can be easily compared in similarity of their (distributional) semantic value.
Models based on vector embeddings have successfully been applied to a variety of 
problems, i.e. semantic search, language translation, question answering and many others.

\section{Self-Attention}
The most notable innovation in the recently introduced large language models (LLMs) called transformers is their heavy use of the attention mechanism. Intuitively, these models use a trainable set of parameters to weigh all pairwise connections between all individual tokens within a context window. These weights are then used to estimate the most likely token being observed in a given context of tokens.
In a process called token embedding, each token is mapped to a high-dimensional vector representation. For each positional slot in the context window a unique constant value is added to these embeddings, disambiguing the different positions a token can occur in.
Each of these embeddings is a point in a hyperdimensional space, representing each of the tokens at a given position.
For calculating the attention relationship between two positional token embeddings, these embeddings are mapped to two different spaces using linear transformation matrices, resulting in points representing both the incoming and outgoing roles in the pairwise attention relationship. The attention value from one positioned token to another is then essentially given by the dot product between these two representations.
Optimizing these linear transformations to maximize the attention weights for all token pairs within the training sequences moves these transformed points closer together for pairs of positioned tokens which appear more frequently.

We have described fundamental problems arising from language models based on neural networks and word embeddings, namely these models struggle with the following qualities:



