
Plan:
\begin{itemize}
\item define target operations on structure as solution to problems
\item define string containment index with these operations
\item define compressed index for efficient structure
\item define how to predict context given tokens
\end{itemize}


Target operations:
\begin{itemize}
\item get all contexts for any substring of corpus
\item reconstruct corpus
\item change corpus after training
\end{itemize}

\section{Problem}
\begin{itemize}
    \item NLMs suffer from hallucinations
    \item NLMs are hard to explain
    \item NLMs are hard to update
    \item NLMs are hard to predict
    \item The need for controllable language models
\end{itemize}
\section{Solution}
\begin{itemize}
    \item use a model without hallucinations for reconstrucing a training corpus
    \item use an explicit symbolic model to help with explainability and updates
    \item allow for creative outputs under controlled conditions
\end{itemize}

\section{Graphics}
\begin{itemize}
    \item visualize attention matrix of transformers
    \item ambiguous grammar to shared packet parse forest
\end{itemize}

\section{Background}

\subsection{Information Retrieval}

\subsection{Hallucinations and Creativity}
\begin{itemize}
    \item models can generate inconsistent outputs
    \item required tradeoff to enable novel creative outputs
\end{itemize}

\subsection{N-Gram Language models}
Early research of language models, 
\begin{itemize}
    \item no hallucinations or creativity, instead issues with sparsity
    \item long range relationships require large n-grams with high cost
    \item hierarchical n-grams can use multiple context windows widths
    \item use markov processes to predict new tokens
    \item can be smoothed to estimate unknown sequences for creativity, at cost of hallucinations
\end{itemize}
\paragraph{Katz Back-Off Model}
\begin{itemize}
    \item able to find largest n-gram to predict next token
\end{itemize}
\paragraph{Markov-Processes}
\begin{itemize}
    \item only require looking at one node at a time to make prediction
\end{itemize}
\paragraph{Probabilistic Context-Free Grammars}
\begin{itemize}
    \item apply stochastic weighing to grammar parsing and generation
\end{itemize}
\paragraph{Counting}
\begin{itemize}
    \item 
\end{itemize}
\subsection{Compression}
\begin{itemize}
    \item relies on replacing repetitions with special symbols to avoid redundancy
    \item useful to decrease hierarchical n-gram size
    \item compressed representation is a grammar generating the uncompressed input
    \item form of grammar induction
\end{itemize}
\subsection{Grammar Induction}
\begin{itemize}
    \item
\end{itemize}
\subsection{Explainability of Language Models}
\begin{itemize}
    \item provides explanations for predictions
    \item makes it easy to find cause for behavior
\end{itemize}

\begin{itemize}
    \item \textbf{Induction:} Construct new rules to represent a given string of symbols
    \item \textbf{Search:} Find grammar rules related to a given string of symbols
\end{itemize}


\paragraph{GLR-Parser}
The Generalized Left Right (GLR) Parser is a parsing algorithm that is able to parse 
ambiguous
and non-deterministic grammars \cite{lang1974deterministic}. It is able to do so by tracking multiple parse trees in a graph-structured stack.

\paragraph{Shared-Packet Parse Forests}

\paragraph{Grammar Induction}

\section{Related Methods}

\paragraph{Sequitur}
The Sequitur algorithm \cite{nevill1997identifying} is a method of grammar induction 
usable for data compression and discovery of lexical structure. The algorithm 
recursively replaces repeated sections in a given input string with new grammar rules, iteratively extending a context-free grammar yielding the original string. The Sequitur algorithm enforces two constraints during its operation:
\begin{itemize}
    \item \textit{Digram Uniqueness}: each subsequent pair of symbols occurs at most once in all rules of the grammar. This constraint prevents redundant digrams in the grammar's rules.
    \item \textit{Rule Utility}: Every rule is used at least twice. This constraint prevents redundant rules.
\end{itemize}
These constraints not only reduce the size of the grammar, allowing for compression, but also capture some of the lexical structure in the original string. Although the algorithm operates in linear space and time, it does not capture all lexical relationships of repeated substrings if there exist overlapping repetitions in the original string. The Sequitur algorithm always replaces the left-most repetition with a non-terminal symbol and does not consider any repetitions starting from within this replaced substring. This causes relationships of some repeated phrases to not be captured, which poses a problem when looking for the complete distribution of contexts of arbitrary sequences, as not all sequences are fully linked with their contexts.
To relieve this issue we introduce a new constraint on the grammar induction, which enforces that for each 
\paragraph{Re-Pair}
\paragraph{R-Trees}
\paragraph{Top-Trees}
\paragraph{Byte-Pair Encoding}
\paragraph{Sequence Alignment}
\paragraph{Hierarchical N-Grams}
\paragraph{Recursive Hypergraph}

\section{Analysis}
\subsection{Search}
\subsection{Insert}

\section{Application}
\subsection{Reconstruction}
\subsection{Update Training Data}
\subsection{Reconstruct Context}
\subsection{Similarity}
\subsection{Predict Context}

\newcommand{\convexpath}[2]{
[   
    create hullnodes/.code={
        \global\edef\namelist{#1}
        \foreach [count=\counter] \nodename in \namelist {
            \global\edef\numberofnodes{\counter}
            \node at (\nodename) [name=hullnode\counter,draw=none] {};
        }
        \node at (hullnode\numberofnodes) [name=hullnode0,draw=none] {};
        \pgfmathtruncatemacro\lastnumber{\numberofnodes+1}
        \node at (hullnode1) [name=hullnode\lastnumber,draw=none] {};
    },
    create hullnodes
]
($(hullnode1)!#2!-90:(hullnode0)$)
\foreach [
    evaluate=\currentnode as \previousnode using \currentnode-1,
    evaluate=\currentnode as \nextnode using \currentnode+1
    ] \currentnode in {1,...,\numberofnodes} {
  let
    \p1 = ($(hullnode\currentnode)!#2!-90:(hullnode\previousnode)$),
    \p2 = ($(hullnode\currentnode)!#2!90:(hullnode\nextnode)$),
    \p3 = ($(\p1) - (hullnode\currentnode)$),
    \n1 = {atan2(\y3,\x3)},
    \p4 = ($(\p2) - (hullnode\currentnode)$),
    \n2 = {atan2(\y4,\x4)},
    \n{delta} = {-Mod(\n1-\n2,360)}
  in 
    {-- (\p1) arc[start angle=\n1, delta angle=\n{delta}, radius=#2] -- (\p2)}
}
-- cycle
}
\newcommand{\nodes}[2][0]{
    %\StrCount{#1,}{,}[\len]
    \global\edef\namelist{#2}
    \foreach[count=\ci] \chr in \namelist {
        %\StrChar{#1}{\ci}[\chr],
        \node(\chr) at (\ci, #1) {\chr};
    }
}
\def\splicelist#1{
\StrCount{#1}{,}[\numofelem]
\ifnum\numofelem>0\relax
    \StrBefore[1]{#1}{,}[\listhead]%
    \StrBehind[1]{#1}{,}[\listtail]%
    \StrBehind[\numofelem]{#1}{,}[\listlast]%
\else
    \let\listhead#1%
    \let\listlast#1%
    \def\listtail{N/A}
\fi
}
\newcommand{\hyperedge}[1]{
    \def\namelist{#1},
    \splicelist{\namelist}
    \draw \foreach \n [remember=\n as \lastn(initially \listhead)] in \listtail {
        (\lastn) edge[->] (\n)
    };
}

\begin{figure}
\centering
\begin{tikzpicture}[
    y=.7cm, x=1cm,
    every edge/.append style={thick}
]

%\node (b) at (1,0) {b};
%\node (c) at (2,0) {c};
%\node (d) at (3,0) {d};
%\node (e) at (4,0) {e};
%\node (f) at (5,0) {f};
\nodes{a, b, c, d, e, f}
\hyperedge{a, b, c, d, e, f}

\filldraw[line width=.5mm, blue,opacity=0.3] \convexpath{a, b, c, d}{8pt};
\filldraw[line width=.5mm, red,opacity=0.3] \convexpath{c, d, e, f}{8pt};

\end{tikzpicture}
\caption{A hypergraph with two hyperedges}
\end{figure}

\subsection{Tracing Overlaps}

To find these overlaps as efficiently as possible, we perform a search locally around symbols we have
previously found. This reduces the search space and makes use of previously computed results.

After we have parsed the first largest symbol, we want to find the next largest overlap with this symbol.
So we search top-down through all of the postfixes of the symbol from largest to smallest, and for each
we try to parse the word resulting from appending the postfix with the remaining tokens from the input word.
We either never find a symbol representing any overlap, in which case we simply continue parsing the remaining input,
or we find an overlap. An overlap starts with some postfix from the former symbol, i.e. it
uses it as a prefix in one of its rules. To build the resulting set of production rules, we need to complete the
context in backward direction, to represent the sequence before the overlap. For this we can make use of the parse
tree of the former symbol of which we searched a postfix. By accumulating the backward context along all of the
levels of its parse tree, we can create a new production rule representing the backward context of the overlap. (PROOF)

In the resulting state we have two bands or two production rules, each representing a prefix of the input word of
different lengths. We now continue to search for overlaps of the now latest symbol, i.e. the former overlap we have
just found. Overlaps for this symbol could also overlap with the symbol we were previously finding overlaps for.
However we can disprove that this can be the case if the production rule of the overlapping entry, i.e. the one
starting with the postfix of the previous symbol, contains more than two symbols. This follows from the condition 
that no sequence of symbols may appear twice anywhere in the entire grammar. Only if there is exactly one symbol 
following the former postfix, may that same sequence appear in another larger symbol. We can use this knowledge to 
decide our next steps.

In the case where there are at least two symbols following the first in the rule, we know that any overlaps must begin 
after the end of the last symbol we overlapped. If there is only one following symbol, however, there may be an overlap 
with the current symbol and also the previous symbol. If such a symbol exists, there must be a production rule in the 
current symbol with a postfix larger than that of the rule we entered the current symbol through, because of the 
maximality constraint. We can use this postfix to find the next overlap, however the postfix may not be expandable into 
the following tokens. We search throught the postfixes in descending order. We will eventually encounter the postfix of 
the rule we entered through. We use it to complete the rule with the first symbol we parsed by appending it. If the 
postfix is expandable, we append the extension. This extension is then overlapping with the current symbol and we can 
move on to repeat the algorithm inside the extension.

Earlier, we mentioned the case where the entry rule of a symbol contained more than two symbols, and that in this case 
we know that there can be no overlap with the current symbol starting at or before the end of the last symbol. But there 
can still be an overlap with the current symbol which starts after the end of the last symbol. In case we don't find any 
overlaps, we will simply create a new symbol for the postfix of the current symbol after the entry from the last symbol 
and append it to the band with the last symbol. We get two bands with different rules but generating the same strings 
(which are of equal length). We create a new symbol for these two bands and replace them with it, because we can not have 
two boundaries at the same string position in any rules of one symbol. 