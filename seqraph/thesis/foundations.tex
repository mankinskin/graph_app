
\chapter{Foundations}

\section{Language Modelling}
\dfn{Language Model}{
    Let $\Sigma$ denote an alphabet of tokens. A language model $M$ assigns a string of tokens $w_1, \ldots, w_n \in \Sigma^n$ an estimated probability $P(w_1, \ldots, w_n)$ or $P(w_i \mid w_1, \ldots, w_{i-1})$ for all $1 \leq i \leq n$, based on distributions seen in a \textbf{training corpus} $T \in \Sigma^N$ of length $N$.\par
}

A language model considers a string of tokens $w = w_1, \ldots, w_n$ as a time-discrete stochastic process $X_1, \ldots, X_n$, where each token position is a random variable over an alphabet (also called vocabulary) $\Sigma$. The model estimates the probability of a given string $w$ by estimating the assignments to these random variables. We write $P(X_1 = w_1, X_2 = w_2, \ldots, X_n = w_n)$ as $P(w_1, \ldots, w_n)$.
Using the chain rule of probabilities we can show that~\cite{parsing2009speech}:
\begin{align*}
    P(w_1, \ldots, w_n) = P(w_1)*P(w_2 \mid w_1)*\ldots*P(w_n \mid w_1, \ldots, w_{n-1})
\end{align*}

\noindent This means that a language model both models the conditional probabilities $P(w_i \mid w_1, \ldots, w_{i-1})$ for all $1 \leq i \leq n$ and the probabilities of given sequences. Another way to see this relationship is the definition of conditional probabilities:
\begin{align*}
    P(w_i \mid w_1, \ldots, w_{i-1}) = \frac{P(w_1, \ldots, w_i)}{P(w_1, \ldots, w_{i-1})}
\end{align*}

A very basic language model is the \textit{N-Gram Model}.
\dfn{N-Gram Model}{
    An N-gram of order $n$ is a string $w^n$ of $n$ tokens. An \textit{N-gram model} stores counts of all N-grams appearing in a given corpus $T \in \Sigma^*$. A count function $C(\cdot): \Sigma^n \rightarrow \NN$ maps each N-gram to the number of times it occurs in $T$. The probability of a token $w_i$ following $n-1$ given tokens can then be calculated using the N-gram counts:
    \begin{align*}
        P(w_i \mid w_{i-n+1}, \ldots, w_{i-1}) = \frac{C(w_{i-n+1}, \ldots, w_i)}{C(w_{i-n+1}, \ldots, w_{i-1})}
    \end{align*}
}
Intuitively, an N-Gram model counts the portion of $(n-1)$-grams in the corpus which have been followed by the token $w_i$. This way it predicts the token at position $i$ from the given context of $n-1$ tokens. The N-Gram model is however limited by $n$ in how much preceding context it can include during prediction. Relationships beyond this context window are not captured by an N-Gram model, which is one of its major drawbacks, as natural language contains long-range dependencies.

Increasing the value of $n$ can quickly increase the number of counts having to be stored, as the number of N-grams to be counted can grow exponentially. The number of possible N-grams over an alphabet $\Sigma$ of size $|\Sigma| = k$ is given by $k^n$. Although a corpus $T$ of length $N$ can only contain at most $N - n + 1$ $n$-grams for a given length $n$, large corpora are long enough for the number of possible N-grams to be counted to become unpractical.

To precisely model the probabilty of a token given an arbitrary context, N-Gram models would need to store N-grams of arbitrary length. As this would often require a lot of space to store the various possible N-Grams, usually the N-Gram length is limited. It is possible to estimate the true probability of each N-Gram given in the corpus by only looking at a limited context size, which is called a Markov assumption:
\dfn{Markov Property}{
    A stochastic process $X_1, \ldots, X_N$ is said to possess the Markov property of order $n$ if:
    \begin{align*}
        P(X_i \mid X_{1}, \ldots, X_{i-1}) = P(X_i \mid X_{i - n}, \ldots, X_{i-1})
    \end{align*}
    That is the probability of the $i$-th state only depends on the past $n$ states. A stochastic process with this property is also called an $n$-th order Markov Chain.
}
A Markov chain of order $1$ is also called \textit{memoryless}, as its next state only depends on the current state. N-Gram models make the assumption that the stochastic process of the language they are modelling possesses the Markov property and that the probability of a token conditioned on the entire context can be approximated with a limited context window:
\begin{align*}
    P(w_i \mid w_{1}, \ldots, w_{i-1}) \approx P(w_i \mid w_{i - n + 1}, \ldots, w_{i-1})
\end{align*}

\section{Attention Mechanism}
N-Gram models have previously been superceded by recurrent neural networks (RNNs), LSTM (Long-Short Term Memory) networks and Transformers, a self-attention based neural architecture. Transformers are designed to \textit{transform} a sequence of input tokens into another by essentially incorporating context information from the input tokens into each output token. In transformers, each input token is represented as a \textit{word embedding}, which is a high-dimensional vector unique for each token. During training, transformers learn \textit{attention weights} between all directed pairs of tokens in the vocabulary, which can be used to produce output vectors incorporating contextual information from input sequences within a limited context window.

\dfn{Scaled Dot-Product Attention}{
    Let $X \in \RR^{n \times d}$ be the matrix of $d$-dimensional word embedding for the $n$ input tokens. Further let $\mathbf{W_K} \in \RR^{d \times d_k}$, $\mathbf{W_Q} \in \RR^{d \times d_k}$ and $\mathbf{W_V} \in \RR^{d \times d_v}$ be the \textit{key}, \textit{query} and \textit{value} weight matrices, mapping each word embedding $x_i$ to its respective key, query and value vectors $k_i$, $q_i$ and $v_i$.
    \begin{align*}
        \mathbf{K} = \mathbf{XW_K},\quad
        \mathbf{Q} = \mathbf{XW_Q},\quad
        \mathbf{V} = \mathbf{XW_V}
    \end{align*}
    Such that $\mathbf{K} \in \RR^{n \times d_k}$, $\mathbf{Q} \in \RR^{n \times d_k}$ and $\mathbf{V} \in \RR^{n \times d_v}$ contain $k_i$, $q_i$ and $v_i$ for all $n$ input tokens in their rows.
    The attention mechanism computes the output embedding for each input token $i$ as a weighted sum of all the value vectors $j$ in $\mathbf{V}$, where the weight is the dot product of the associated $q_i$ and $k_j$ vectors, scaled by the inverse root of the key and query vector dimension:
    \begin{align*}
        \text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)\mathbf{V}
    \end{align*}
}
The associated $q_i$ and $k_i$ vectors can be viewed as positions of the ``sending'' and ``receiving'' endpoints in a ``\textit{link space}'', where the similarity of two vectors corresponds to the attention weight the first token assigns to the other. With this representation, tokens with similar contexts in the training data are assigned similar endpoints in the link space during training. This allows the transformer model to easily generalize from the training data and predict similar context even if they never occurred in the training data, simply based on the similarity of each token's contexts.

The transformer architecture as it was presented in \citetitle{VaswaniSPUJGKP17}\cite{VaswaniSPUJGKP17} trains more than one set of attention weight matrices which it calls \textit{multi-head attention}. Each set of $\mathbf{(W_Q, W_K, W_V)}$ is called an \textit{attention head}. This way the model can learn different assignments of attention weights between tokens, which allows it to learn different types of relationships between tokens and helps learn dependencies over longer distances.

The attention mechanism on word embeddings highlights a key advantage of neural language model architectures over count-based N-gram models. Neural models can generally be applied with more flexibility and need less training data to produce sophisticated results. 